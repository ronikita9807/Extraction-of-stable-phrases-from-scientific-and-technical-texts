from gensim.models import FastTextimport gensimimport pandas as pdimport numpy as npfrom sklearn.decomposition import PCAfrom sklearn.cluster import AgglomerativeClusteringimport pymorphy2from tqdm import tqdmmorph = pymorphy2.MorphAnalyzer()import pymorphy2from collections import Counterimport csvfrom math import log2, sqrt, log10from os import listdirfrom os.path import isfile, joinimport refrom tqdm import tqdmfrom dataclasses import dataclassdef get_words_from_collect(name):    punkt = "абвгдеёжзийклмнопрстуфхцчшщъыьэюяАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ"    def check_syntax(word):        word2 = word        for i in word:            if i not in punkt:                word2 = word2.replace(i, "")        return word2    text = ""    onlyfiles = [f for f in listdir(name) if isfile(join(name, f))]    with tqdm(total=len(onlyfiles), unit="file", mininterval=5) as progress_bar:        for i in onlyfiles:            with open(name + "/" + i, "r", encoding="utf-8", ) as f:                for line in f:                    text += line            progress_bar.update()    split_regex = re.compile(r"[.|!|?|…]")    sentences = list(filter(lambda t: t, [t.strip() for t in split_regex.split(text)]))    real_words = []    with tqdm(total=len(sentences), unit="sent", mininterval=5) as progress_bar:        for sentence in sentences:            for word in sentence.split():                word = check_syntax(word)                p = morph.parse(word)[0]                if p.normal_form != "":                    real_words.append(p.normal_form)            progress_bar.update()    collocs = []    for i in range(len(real_words)-1):        collocs.append(real_words[i]+" "+real_words[i+1])    vec1 = Counter(real_words)    vec2 = Counter(collocs)    print(f"{name} size: {len(real_words)}")    return vec1, vec2data2 = pd.read_csv("sentences_for_training_embeddings(18.04.22).csv")print(data2.head())# arrays = data2["sentences"]arrays = list(data2["arrays"])new_sentences = []with tqdm(total=len(arrays), unit="colloc", mininterval=5) as progress_bar:    for tup in arrays:        a = tup.split(" ")        new_sentences += [[i for i in a]]        progress_bar.update()# print("download model")# model = gensim.models.fasttext.load_facebook_model("ft_native_300_ru_wiki_lenta_lemmatize.bin")# #Если без дообученияmodel = FastText(window=3, min_count=1)model.build_vocab(new_sentences)print("train on new data")# model.build_vocab(new_sentences, update=True)  # Update the vocabularymodel.train(new_sentences, total_examples=len(new_sentences), epochs=model.epochs)print(model.wv.most_similar('решать_задачи'))df = pd.read_csv("sample_for_bert_new_full_info(16.04.2022).csv", sep=',')colloc1 = list(df.norm_form)colloc = []with tqdm(total=len(colloc1), unit="colloc", mininterval=5) as progress_bar:    for tup in colloc1:        r = tup.replace("(", "")        r = r.replace(")", "")        r = r.replace("'", "")        r = r.replace(" ", "")        a = r.split(",")        colloc.append(a[0]+"_"+a[1])        progress_bar.update()class1 = df["class"]j = 0bad_example = []good_example = []with tqdm(total=len(colloc), unit="colloc", mininterval=5) as progress_bar:    for i in class1:        if i:            a = colloc[j]            good_example.append(a)        else:            a = colloc[j]            bad_example.append(a)        j+=1        progress_bar.update()df2 = pd.read_csv("result_bert_after_filter(17.04.22).csv", sep=',')result0 = set(df2.colloc)result = set()for i in result0:    a = i.split(" ")    result.add(a[0]+"_"+a[1])print("\nЧисло словосочетаний после классификаторов:", len(result))bad = set()with tqdm(total=len(bad_example), unit="colloc", mininterval=5) as progress_bar:    for i in bad_example:        candidate = set([item[0] for item in model.wv.most_similar(i, topn=150)])        bad.update(candidate)        progress_bar.update()result.difference_update(bad)result1 = list(result)print("\nЧисло словосочетаний после удаления неустойчивых: ", len(result))a = [morph.parse(i)[0].normal_form for i in result1]result1 = list(set(a))result3 = []for i in result1:    a = i.split("_")    print(a)    if len(a) == 2 and len(a[0]) > 2 and len(a[1]) > 2:        result3.append(a[0]+" "+a[1])        continue    a = i.split(" ")    print(a)    if len(a) == 2 and len(a[0])>2 and len(a[1])>2:        result3.append(a[0]+" "+a[1])        continueprint("\nразмер после фильтрации: ", len(result3))collect = ["ИТ", "Биология", "Физика", "Математика"]collects = {}collects_word = {}for name in collect:    vec1, vec2 = get_words_from_collect(name)    collects[name] = vec2    collects_word[name] = vec1result5 = {}result3 = list(result3)for i in result3:    a, b = i.split(" ")    result5[i] = 0    for name in collects_word:        result5[i] += 1 if collects_word[name][a] > 5 and collects_word[name][b] > 5 else 0result_final = []for i in result5:    if result5[i] > 2:        result_final.append(i)print(f"Финальное количество словосочетаний: {len(result_final)}")df3 = pd.DataFrame({"colloc": list(result_final)})df3.to_csv("new_colloc_after_bert_result(20.04.22).csv")print("Конец!")